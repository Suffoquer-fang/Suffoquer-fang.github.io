---
layout: post
header-style: text
title: 研二上学期总结
tags: 总结
author: Suffoquer
catalog: true
preview: 现在是12.22，从学校跑毒回家结果还是阳了，现在已经发烧第四天了还是没退烧，不管了，烧吧烧吧。躺在床上没事干，来写写总结。
---

现在是12.22，从学校跑毒回家结果还是阳了，现在已经发烧第四天了还是没退烧，不管了，烧吧烧吧。躺在床上没事干，来写写总结。

## 课程
缺8学分，选了4+3+2，亏了一分心痛好久。这学期又可以记两个PF，好像除了研一上以外我几乎都是PF了。真好，反正西贝尔早就没戏了，硕士GPA也没什么用，懒得卷了，没有意义。

人类从历史中学到的唯一教训就是人类不会从历史中吸取教训。我在课上学到的唯一真理就是我上课学不到任何东西。

### 最优化方法（PF）
就去过一次线下课，后面都是雨课堂线上了，反正也没听过。有14次作业，但是好像只占20%。作业就对着PPT和凡老师的作业答案拟合，大部分都是原题。不得不说，这老师的PPT做得确实不太行，直接看感觉好乱，可能是我没听课的问题。
还没考试，不过是开卷。考完再来写

~~考完了，一点也不难，而且PF了我也不在意。~~

至于说学到了什么，印象最深的东西就是凸函数的定义，其他的好像都没学会。凡老师觉得这个对科研还是有帮助的，也许吧。我做（混）的科研大多都跟数学没什么关系，我也不觉得这些数学课能对做检索推荐的人有什么帮助。再说，真有什么需要用的数学，现学大概也能学会吧。

### 机器人认知计算（PF）
佳玉姐推荐的课，无作业无考勤，评分就是最后写个报告。只去了第一节课，后面一次都没有听过，打算到ddl用xiezuocat或者chatgpt胡诌个报告交上去完事，反正PF，我就不信他会挂我。

### 人工智能
马老师的课，主要面向外系的同学介绍人工智能，内容非常基础。讲实话实用价值并不高，要学理论的话朱军的统计机器学习会深入得多，要学应用的话也有自然语言处理或者计算机视觉相应的课。马老师给本科生开的“人工智能导论”和这个内容一样作业一样，所以才选了，作业改下学号直接交就行。

课程有随堂小测，不过我都没做。三次大作业和一次考试，三次任选一次做就可以了。大作业是拼音输入法，重力四子棋AI和mnist手写数字识别。以前第三次是文本情感分类，改成这个更简单了，直接用chatgpt写肯定都没问题。

考试可以带两张A4纸的小抄，按照本科经验难度应该不大，不过我现在应该比本科时候退化很多了。涉及的考点就那几个，会有一个开放一点的题，我是瞎写的。

## 科研
本来不想写这部分的，我哪有在科研，我只是在完成任务让老板看起来我在做事而已。一直在搞Dense Retrieval迁移的事情，纯粹就是在不停尝试各种做法，最后总算碰上一个看起来有效果的东西。本来是借鉴了之前一篇工作（Match-Prompt）的做法，用在DR上好像也还行。谁知道12月的时候发现人家作者自己也用在DR上又搞了一篇，方法上和我的几乎一样比我高级得多。没办法，祈祷审稿人没看过那篇吧。

别的没什么好写的，现在在赶论文，要是这篇赶不上的话，能拖一拖改一改当硕士毕设也挺好的。

## 找工
这学期一开始我就在焦虑秋招的事情了，预想了很多要做的事情，不过大多烂尾了。烂尾贯穿我的人生，烂尾就是我的常态，这篇能写到这里还没烂尾真是奇迹了。
方向是入学时候就明确了的，优先级应该是大厂ML平台/系统开发 > 量化dev > 大厂算法岗 = 游戏厂开发 = 国企/其他

简单解释一下吧，首先是为什么开发 > 算法，本科失败的科研经历让我觉得自己不适合做探索类的事情（硕士之后更加确信了），每天去思考为什么模型没有效果对我来说是种折磨，学生时期还好，最坏的结果就是被老板骂一顿，要是工作以后几个月没成果估计就要刷题找下家了。相比这样，我宁愿做踏实一点的开发工作，累点就累点，好歹写一行代码是一行。

为什么平台和系统，我觉得AI在未来几十年内肯定还是有很大发展空间的，具体到算法和模型上肯定会快速迭代，但是像AI平台和系统相对来讲会稳定一些，应该不太容易被全面淘汰。

量化纯粹是冲着钱去的，只考虑dev，不考虑quant researcher，原因上面解释过了，我讨厌产出不确定的事情（况且researcher我估计也做不了，看了下tmh的笔试题，就没几题我会做的）。做量化技术路线和大厂不太一样，之后发展会更局限一点，但架不住量化给的是真的多。只考虑外资，稳定一点，搞不好哪天一纸公文，国内就没有量化这个行业了，谁知道呢。

~~（好了，这下全部真香了）*2024.03*~~

9月份做了简历之后发现没有拿得出手的项目和经历，项目全是本科的大作业，没有什么亮点。经历上只有一段某大厂推荐算法岗的实习，做的事情naive到我都不好意思写出来。觉得要搞一些项目，当时选了rCore和TinyKV。rCore能顺便重新学一遍os，写了写发现和面试要问的差太远了，还是老老实实背八股吧，就没继续了。TinyKV想着写完了可以替代掉原来的数据库项目，顺便还带一点分布式的东西，写了两个lab，烂尾了。

Leetcode断断续续在刷，会的还是会，不会的看了题解下次碰到大概率还是不会，感觉没什么长进，还是得专门抽时间集中刷题。

准备先面试几家看看情况，投了腾讯和百度。百度投了深度学习平台开发，一直在简历初筛，应当不至于简历都过不了吧。腾讯投的是后台开发，天美工作室捞起来的，一面挂了，具体面试经过就不写了。看了几家大厂都不招日常实习了，有点头大。准备年后让李哥帮忙内推一下，现在还没时间准备这事。

## 其他
没有什么其他，我的日常就是实验室-宿舍-食堂，娱乐活动几乎只有游戏，偶尔打台球。我没有生活，我只是活着。

总结就写到这里吧，现在记忆力大不如前了，上半年的事好多都想不起来了，以后还是得多写下来。

PS

刚刚试了一下拿chatgpt写人工智能的作业，以下是一段给出的结果：
> 在这份报告中，我们研究了使用卷积神经网络（CNN）在MNIST数据集上进行手写数字识别时，使用不同超参数的效果。
> 首先，我们尝试使用不同的优化器来训练模型，包括Adam和SGD。我们发现，使用Adam优化器训练的模型在测试数据上的准确率为97.75%，而使用SGD优化器训练的模型的准确率为95.39%。因此，在这种情况下，使用Adam优化器训练的模型表现得更好。
> 然后，我们尝试调整卷积层的数量和尺寸。我们发现，将卷积层数从2增加到3可以使模型的准确率提高1.16%，而将卷积核尺寸从3x3增加到5x5则使准确率提高了1.29%。
> 最后，我们尝试调整批量大小。我们发现，将批量大小从32增加到64可以使模型的准确率提高0.79%，而将批量大小从64减小到16则使准确率下降了0.87%。

代码和报告都不用自己写了，至于报告里的数据准不准，我不知道，不过助教也必不可能去跑实验，就算不准又有什么关系呢。